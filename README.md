# DeepLingo
Designed and implemented a full Transformer-based English–Italian NMT system in PyTorch, including multi-head self-attention,
 positional encodings, layer normalization, and an encoder–decoder architecture.
 • Preprocessed and trained on the OPUS Books parallel corpus using byte-pair encoding (32K subword vocabulary), achieving a
 BLEU score of 28.5.
 • Enhanced inference with a beam-search decoder (beam size=5) and built attention heatmap visualizations for qualitative analysis
 of translation alignments.
